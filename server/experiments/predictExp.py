# -*- coding: utf-8 -*-
"""Generating Piano Music with Transformer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/notebooks/magenta/piano_transformer/piano_transformer.ipynb

##### Copyright 2019 Google LLC.

Licensed under the Apache License, Version 2.0 (the "License");

# Generating Piano Music with Transformer
### ___Ian Simon, Anna Huang, Jesse Engel, Curtis "Fjord" Hawthorne___

This Colab notebook lets you play with pretrained [Transformer](https://arxiv.org/abs/1706.03762) models for piano music generation, based on the [Music Transformer](http://g.co/magenta/music-transformer) model introduced by [Huang et al.](https://arxiv.org/abs/1809.04281) in 2018.

The models used here were trained on over 10,000 hours of piano recordings from YouTube, transcribed using [Onsets and Frames](http://g.co/magenta/onsets-frames) and represented using the event vocabulary from [Performance RNN](http://g.co/magenta/performance-rnn).

Unlike the original Music Transformer paper, this notebook uses attention based on absolute instead of relative position; we may add models that use relative attention at some point in the future.

# Environment Setup
"""

# Commented out IPython magic to ensure Python compatibility.
#@title Setup Environment
#@markdown Copy some auxiliary data from Google Cloud Storage.
#@markdown Also install and import Python dependencies needed
#@markdown for running the Transformer models.

# %tensorflow_version 1.x

import ctypes.util

def proxy_find_library(lib):
  if lib == 'fluidsynth':
    return 'libfluidsynth.so.1'
  else:
    return ctypes.util.find_library(lib)
# ctypes.util.find_library = proxy_find_library
# ctypes.util.find_library = 'libfluidsynth.so.1'

print('Importing libraries...')

import numpy as np
import os
import tensorflow.compat.v1 as tf
import tempfile
from io import BytesIO
import pretty_midi

# from google.colab import files

print('importing tensor2tensors')

from tensor2tensor import models
from tensor2tensor import problems
from tensor2tensor.data_generators import text_encoder
from tensor2tensor.utils import decoding
from tensor2tensor.utils import trainer_lib

print('importing magenta')

from magenta.models.score2perf import score2perf
import note_seq

tf.disable_v2_behavior()

print('Done!_____________________________(*(*(*(*(*(*')

#@title Definitions
#@markdown Define a few constants and helper functions.

# global unconditional_samples, input_fn, prime_ns, primer, unconditional_encoders, problem, saveMidiLoc, estimator, problem, _

SF2_PATH = '/home/mrityunjay/Documents/Projects/BitRate/aiDuetWithTransformer/assets/soundFonts/Yamaha-C5-Salamander-JNv5.1.sf2'
saveMidiLoc = '/home/mrityunjay/Documents/Projects/BitRate/aiDuetWithTransformer/assets/genMusic/unconditional.mid';
SAMPLE_RATE = 16000


# Upload a MIDI file and convert to NoteSequence.
def upload_midi():
  return None
  data = list(files.upload().values())
  if len(data) > 1:
    print('Multiple files uploaded; using only one.')
  return note_seq.midi_to_note_sequence(data[0])

# Decode a list of IDs.
def decode(ids, encoder):
  ids = list(ids)
  if text_encoder.EOS_ID in ids:
    ids = ids[:ids.index(text_encoder.EOS_ID)]
  return encoder.decode(ids)

"""# Piano Performance Language Model"""

#@title Setup and Load Checkpoint
#@markdown Set up generation from an unconditional Transformer
#@markdown model.


# Create input generator (so we can adjust priming and
# decode length on the fly).

filenames = {
    'C major arpeggio': '/home/mrityunjay/Documents/Projects/BitRate/aiDuetWithTransformer/assets/midi/c_major_arpeggio.mid',
    'C major scale':    '/home/mrityunjay/Documents/Projects/BitRate/aiDuetWithTransformer/assets/midi/c_major_scale.mid',
    'Clair de Lune':    '/home/mrityunjay/Documents/Projects/BitRate/aiDuetWithTransformer/assets/midi/clair_de_lune.mid',
}
primer = 'C major scale'  #@param ['C major arpeggio', 'C major scale', 'Clair de Lune', 'Upload your own!']

global targets
global decode_length

model_name = 'transformer'
hparams_set = 'transformer_tpu'
ckpt_path = '/home/mrityunjay/Documents/Projects/BitRate/aiDuetWithTransformer/assets/checkpoints/unconditional_model_16.ckpt'

class PianoPerformanceLanguageModelProblem(score2perf.Score2PerfProblem):
  @property
  def add_eos_symbol(self):
    return True

problem = PianoPerformanceLanguageModelProblem()
unconditional_encoders = problem.get_feature_encoders()

# Set up HParams.
hparams = trainer_lib.create_hparams(hparams_set=hparams_set)
trainer_lib.add_problem_hparams(hparams, problem)
hparams.num_hidden_layers = 16
hparams.sampling_method = 'random'

# Set up decoding HParams.
decode_hparams = decoding.decode_hparams()
decode_hparams.alpha = 0.0
decode_hparams.beam_size = 1

# Create Estimator.
run_config = trainer_lib.create_run_config(hparams)
estimator = trainer_lib.create_estimator(
    model_name, hparams, run_config,
    decode_hparams=decode_hparams)




def generate_midi(midi_data, total_seconds=10):

    print('init prints: ', model_name, hparams_set, ckpt_path)

    def input_generator():
      print('inside input_gen')
      # These values will be changed by subsequent cells.
      while True:
        yield {
            'targets': np.array([targets], dtype=np.int32),
            'decode_length': np.array(decode_length, dtype=np.int32)
        }

    targets = []
    decode_length = 0

    # Start the Estimator, loading from the specified checkpoint.
    input_fn = decoding.make_input_fn_from_generator(input_generator())

    unconditional_samples = estimator.predict(
        input_fn, checkpoint_path=ckpt_path)

    # "Burn" one.
    _ = next(unconditional_samples)
    # prime_ns = note_seq.midi_file_to_note_sequence(filenames[primer])

    print(unconditional_encoders, unconditional_samples, _, input_fn)


    # print(
    #     'inside generate midi', unconditional_samples,input_generator
    # )
    prime_ns = note_seq.midi_io.midi_to_sequence_proto(midi_data)

    # Handle sustain pedal in the primer.
    primer_ns = note_seq.apply_sustain_control_changes(prime_ns)

    targets = unconditional_encoders['targets'].encode_note_sequence(
        primer_ns)

    # Remove the end token from the encoded primer.
    targets = targets[:-1]

    decode_length = max(0, 4096 - len(targets))
    if len(targets) >= 4096:
      print('Primer has more events than maximum sequence length; nothing will be generated.')

    # Generate sample events.
    sample_ids = next(unconditional_samples)['outputs']

  
    # Decode to NoteSequence.
    midi_filename = decode(
        sample_ids,
        encoder=unconditional_encoders['targets'])
    ns = note_seq.midi_file_to_note_sequence(midi_filename)

    # Append continuation to primer.
    continuation_ns = note_seq.concatenate_sequences([primer_ns, ns])

    note_seq.sequence_proto_to_midi_file(
        continuation_ns, saveMidiLoc)
    print('finished generating.... returning the final file') 

    return saveMidiLoc 

print('it works!!!')



valStr = 'MThd\x00\x00\x00\x06\x00\x00\x00\x01\x01àMTrk\x00\x00\x00,\x00ÿQ\x03\x07¡ \x00\x90,\x7f\x81,\x901\x7f\x00\x80,Z$\x903\x7f\x00\x801Z*\x906\x7f\x00\x803Z$\x806Z\x00ÿ/\x00'

valByt = BytesIO(bytes(valStr, 'latin1'))
midi_data = pretty_midi.PrettyMIDI(valByt)
# duration = float(request.args.get('duration'))
print('setting retMidi')

generate_midi(midi_data)
